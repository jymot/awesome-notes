# 本地部署R1模型

### 环境准备
 * 创建虚拟环境
```bash
conda create --name py311 python=3.11
conda init
source ~/.bashrc
conda activate py311
```
 * 如果需要使用jupyter，那么安装如下依赖
 ```bash
 conda install jupyterlab
 conda install ipykernel
 python -m ipykernel install --user --name py311 --display-name "Python 3.11"
 ```

### 安装
 * clone项目
```bash
# 下载V3模型工程，需要使用V3模型工程中inference推理文件夹中的脚本来完成R1模型的调用(因为R1模型没有单独的脚本，它们的调用方式相同)
git clone https://github.com/deepseek-ai/DeepSeek-V3.git

# 进入inference推理文件夹，安装依赖
cd DeepSeek-V3/inference
pip install -r requirements.txt
```

### 下载模型权重
可以HuggingFace或者ModelScope下载，国内推荐ModelScope。R1模型权重下载地址：https://www.modelscope.cn/models/deepseek-ai/DeepSeek-R1

 * 安装ModelScope
```bash
pip install modelscope
```

 * 下载模型权重
```bash
# 创建models/deepseek-ai/DeepSeek-R1目录
mkdir -p ./models/deepseek-ai/DeepSeek-R1

# 下载模型权重，下载时间会很长，可以使用会话持久命令保持下载进程，比如nohup，screen等
modelscope download --model deepseek-ai/DeepSeek-R1 --local_dir ./models/deepseek-ai/DeepSeek-R1
```

 * 借助convert.py模型权重转化
 ```bash
 cd DeepSeek-V3/inference
 python convert.py --hf-ckpt-path /path/to/DeepSeek-R1 --save-path /path/to/DeepSeek-R1-Demo --n-experts 256 --model-parallel 16

 # 参数说明
 # python convert.py：运行 convert.py脚本，这个脚本的作用是将Hugging Face格式的模型权重转换为另一个特定的格式
 # --hf-ckpt-path /path/to/DeepSeek-R1：指定Hugging Face格式的模型检查点路径（即模型 文件所在的位置）。在这个例子中是 /path/to/DeepSeek-R1
 # --save-path /path/to/DeepSeek-R1-Demo：指定转换后的模型权重保存的路径，即你想将模型 保存到哪里
 # --n-experts 256：指定模型中的专家数量，这可能是与模型的分布式训练或者混合专家模型 （Mixture of Experts, MoE）相关的参数，设置为256表示该模型将使用256个专家。
 # --model-parallel 16：这个参数指定模型并行的程度，通常是指将模型分布在16个GPU上进行 训练或推理。
 ```


 * 借助generate.py进行模型推理
 ```bash
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR 
generate.py --ckpt-path /path/to/DeepSeek-R1-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200
 # 参数说明
 # torchrun ：用于分布式训练或推理的命令。它会启动多个进程，支持跨多个节点（即多台机器） 运行
 # --nnodes 2 ：指定集群中的节点数目。在这个例子中，使用2个节点（即2台机器）。
 # --nproc-per-node 8 ：每个节点上运行的进程数。在每个节点上启动8个进程，通常是为了充分 利用每个节点上的多个GPU。
 # --node-rank $RANK ：当前节点在集群中的排名，用于区分不同节点。 $RANK 是一个环境变量， 代表当前节点的编号。 
 # --master-addr $ADDR ：指定主节点的地址。主节点通常负责协调所有其他节点的工作。 $ADDR 是一个环境变量，表示主节点的IP地址或主机名。
 # generate.py ：这是运行文本生成任务的脚本，它会使用模型进行推理生成文本
 # --ckpt-path /path/to/DeepSeek-R1-Demo ：指定转换后的DeepSeek-R1模型检查点路径。这 个路径是你之前在转换步骤中保存的模型文件。
 # --config configs/config_671B.json ：指定模型的配置文件。配置文件包含模型的架构、超参 数等信息。在这个例子中，配置文件是 configs/config_671B.json。
 # --interactive ：启用交互模式，通常表示用户可以在命令行中输入内容，并实时获得模型的响 应。
 # --temperature 0.7 ：生成的温度值，影响模型的输出随机性。温度越高，生成的文本越随机； 温度越低，生成的文本越确定。 0.7通常意味着适度的随机性。
 # --max-new-tokens 200 ：限制生成的最大新token数量，表示每次生成时最多生成200个新的词 元。
 ```

### 使用SGLang部署
SGLang 目前支持 MLA 优化、 DP Attention、 FP8 (W8A8)、 FP8 KV 缓存和 Torch Compile，在开 源框架中提供了领先的延迟和吞吐量性能。

需要注意的是， SGLangv0.4.1 完全支持在 NVIDIA 和 AMD GPU 上运行 DeepSeek-V3和R1，使其 成为一个高度通用且稳健的解决方案。 SGLang 还支持多节点张量并行，允许你在多个网络连接的机器上 运行该模型。目前多标记预测（MTP）正在开发中，进展可以在优化计划中追踪。

 * 安装SGLang
```bash
pip install --upgrade pip
pip install sgl-kernel --force-reinstall --no-deps
pip install "sglang[all]>=0.4.1.post5" --find-links https://flashinfer.ai/whl/cu124/torch2.4/flashinfer
```
 
 * 调用DeepSeek R1
```bash
# --model 指定模型路径
# --tp 指定推理的设备
# --trust-remote-code 信任远程代码
python3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1 --tp 8 --trust-remote-code
```
执行成功后，默认使用3000端口，然后就可使用OpenAI风格API来调用DeepSeek R1模型了：
```python
import openai
client = openai.Client( 
    base_url="http://127.0.0.1:30000/v1", 
    api_key="EMPTY")
# Chat completion
response = client.chat.completions.create(
    model="default",
    messages=[ 
        {"role": "system", "content": "You are a helpful AI assistant"},
        {"role": "user", "content": "List 3 countries and their capitals."},
    ], 
    temperature=0, 
    max_tokens=64, 
)
print(response)
```

### 使用LMDeploy部署
LMDeploy 是一个灵活且高性能的推理与服务框架，专为大语言模型量身定制，现在支持DeepSeek-R1。它提供了离线管道处理和在线部署能力，能够与基于 PyTorch 的工作流无缝集成。具体 使用LMDeploy调用DeepSeek R1流程如下：

 * 安装 LMDeploy
 ```bash
 git clone -b support-dsv3 https://github.com/InternLM/lmdeploy.git 
 cd lmdeploy
 pip install -e .
 ```

 * 单任务推理，编写Python脚本
 ```python
 from lmdeploy import pipeline, PytorchEngineConfig
 if __name__ == "__main__":
    pipe = pipeline("deepseek-ai/DeepSeek-R1-FP8", 
    backend_config=PytorchEngineConfig(tp=8)) 
    messages_list = [ 
        [{"role": "user", "content": "Who are you?"}], 
        [{"role": "user", "content": "Translate the following content into Chinese directly: DeepSeek-V3 adopts innovative architectures to guarantee economical training and efficient inference."}],
        [{"role": "user", "content": "Write a piece of quicksort code in C++."}], 
        ] 
    output = pipe(messages_list) 
    print(output)
 ```

  * 在线服务调用
  ```bash
  lmdeploy serve api_server deepseek-ai/DeepSeek-R1-FP8 --tp 8 --backend pytorch
  ```
  启动服务后，默认使用23333端口，然后使用OpenAI风格调用即可:
  ```python
  from openai import OpenAI 
  client = OpenAI( 
    api_key='YOUR_API_KEY', 
    base_url="http://0.0.0.0:23333/v1")
  model_name = client.models.list().data[0].id 
  response = client.chat.completions.create( 
    model=model_name, 
    messages=[ {"role": "user", "content": "Write a piece of quicksort code in C++."} ], 
    temperature=0.8, 
    top_p=0.8)
  print(response)
  ```

### 使用vLLM部署
vLLM v0.6.6 支持在 NVIDIA 和 AMD GPU 上以 FP8 和 BF16 模式进行 DeepSeek-R1 推理。除了标准技 术外，vLLM 还提供了管道并行性，允许你在多个网络连接的机器上运行该模型。

 * 安装vLLM
 ```bash
 pip install vllm
 ```

 * DeepSeek R1调用
 目前vLLM已支持DeepSeek R1模型调用，可以在模型支持列表中查看模型关键字: [https://docs.vllm.ai/en/latest/models/supported_models.html](https://docs.vllm.ai/en/latest/models/supported_models.html)

 接下来即可使用如下代码进行调用：
 ```python
 from vllm import LLM
 # For generative models (task=generate) only
 llm = LLM(model=deepseek-ai/DeepSeek-R1, task="generate") 
 # 这里需要手动修改R1模型权重保存路径
 output = llm.generate("Hello, my name is") 
 print(output)
 ```