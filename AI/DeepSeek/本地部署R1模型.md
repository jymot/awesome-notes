# 本地部署R1模型

### 环境准备
 * 创建虚拟环境
```bash
conda create --name py311 python=3.11
conda init
source ~/.bashrc
conda activate py311
```
 * 如果需要使用jupyter，那么安装如下依赖
 ```bash
 conda install jupyterlab
 conda install ipykernel
 python -m ipykernel install --user --name py311 --display-name "Python 3.11"
 ```

### 安装
 * clone项目
```bash
# 下载V3模型工程，需要使用V3模型工程中inference推理文件夹中的脚本来完成R1模型的调用(因为R1模型没有单独的脚本，它们的调用方式相同)
git clone https://github.com/deepseek-ai/DeepSeek-V3.git

# 进入inference推理文件夹，安装依赖
cd DeepSeek-V3/inference
pip install -r requirements.txt
```

### 下载模型权重
可以HuggingFace或者ModelScope下载，国内推荐ModelScope。R1模型权重下载地址：https://www.modelscope.cn/models/deepseek-ai/DeepSeek-R1

 * 安装ModelScope
```bash
pip install modelscope
```

 * 下载模型权重
```bash
# 创建deepseek目录
mkdir ./deepseek

# 下载模型权重，下载时间会很长，可以使用会话持久命令保持下载进程，比如nohup，screen等
modelscope download --model deepseek-ai/DeepSeek-R1 --local_dir ./deepseek
```

 * 借助convert.py模型权重转化
 ```bash
 cd DeepSeek-V3/inference
 python convert.py --hf-ckpt-path /path/to/DeepSeek-R1 --save-path /path/to/DeepSeek-R1-Demo --n-experts 256 --model-parallel 16

 # 参数说明
 # python convert.py：运行 convert.py脚本，这个脚本的作用是将Hugging Face格式的模型权重转换为另一个特定的格式
 # --hf-ckpt-path /path/to/DeepSeek-R1：指定Hugging Face格式的模型检查点路径（即模型 文件所在的位置）。在这个例子中是 /path/to/DeepSeek-R1
 # --save-path /path/to/DeepSeek-R1-Demo：指定转换后的模型权重保存的路径，即你想将模型 保存到哪里
 # --n-experts 256：指定模型中的专家数量，这可能是与模型的分布式训练或者混合专家模型 （Mixture of Experts, MoE）相关的参数，设置为256表示该模型将使用256个专家。
 # --model-parallel 16：这个参数指定模型并行的程度，通常是指将模型分布在16个GPU上进行 训练或推理。
 ```


 * 借助generate.py进行模型推理
 ```bash
torchrun --nnodes 2 --nproc-per-node 8 --node-rank $RANK --master-addr $ADDR 
generate.py --ckpt-path /path/to/DeepSeek-R1-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200
 # 参数说明
 # torchrun ：用于分布式训练或推理的命令。它会启动多个进程，支持跨多个节点（即多台机器） 运行
 # --nnodes 2 ：指定集群中的节点数目。在这个例子中，使用2个节点（即2台机器）。
 # --nproc-per-node 8 ：每个节点上运行的进程数。在每个节点上启动8个进程，通常是为了充分 利用每个节点上的多个GPU。
 # --node-rank $RANK ：当前节点在集群中的排名，用于区分不同节点。 $RANK 是一个环境变量， 代表当前节点的编号。 
 # --master-addr $ADDR ：指定主节点的地址。主节点通常负责协调所有其他节点的工作。 $ADDR 是一个环境变量，表示主节点的IP地址或主机名。
 # generate.py ：这是运行文本生成任务的脚本，它会使用模型进行推理生成文本
 # --ckpt-path /path/to/DeepSeek-R1-Demo ：指定转换后的DeepSeek-R1模型检查点路径。这 个路径是你之前在转换步骤中保存的模型文件。
 # --config configs/config_671B.json ：指定模型的配置文件。配置文件包含模型的架构、超参 数等信息。在这个例子中，配置文件是 configs/config_671B.json。
 # --interactive ：启用交互模式，通常表示用户可以在命令行中输入内容，并实时获得模型的响 应。
 # --temperature 0.7 ：生成的温度值，影响模型的输出随机性。温度越高，生成的文本越随机； 温度越低，生成的文本越确定。 0.7通常意味着适度的随机性。
 # --max-new-tokens 200 ：限制生成的最大新token数量，表示每次生成时最多生成200个新的词 元。
 ```

